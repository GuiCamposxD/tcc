{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "**Note:** This notebook runs on ml.t3.medium (cheap). Heavy training runs on ml.g4dn.xlarge (GPU) via SageMaker Jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Configurations\n",
    "\n",
    "Prepare data locally (runs fast on t3.medium), then upload to S3 for training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SageMaker setup\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sess.default_bucket()\n",
    "prefix = 'fashion-bacnn'\n",
    "\n",
    "print(f'SageMaker role: {role}')\n",
    "print(f'S3 bucket: {bucket}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset\n",
    "path = kagglehub.dataset_download(\"paramaggarwal/fashion-product-images-dataset\")\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and filter dataset\n",
    "styles_csv_path = f'{path}/fashion-dataset/styles.csv'\n",
    "styles_df = pd.read_csv(styles_csv_path, on_bad_lines='skip')\n",
    "\n",
    "# Filter by masterCategory\n",
    "top_3_master = styles_df['masterCategory'].value_counts().nlargest(3).index.tolist()\n",
    "filtered_df = styles_df[styles_df['masterCategory'].isin(top_3_master)]\n",
    "\n",
    "# Filter by subCategory\n",
    "result_df = pd.DataFrame()\n",
    "for master_cat in top_3_master:\n",
    "    df_master = filtered_df[filtered_df['masterCategory'] == master_cat]\n",
    "    top_sub = [sc for sc in df_master['subCategory'].value_counts().nlargest(5).index.tolist() if sc != \"Watches\"][:2]\n",
    "    result_df = pd.concat([result_df, df_master[df_master['subCategory'].isin(top_sub)]])\n",
    "\n",
    "# Filter by articleType\n",
    "dataset = pd.DataFrame()\n",
    "for sub_cat in result_df['subCategory'].unique():\n",
    "    if sub_cat in ['Watches', 'Flip Flops']:\n",
    "        continue\n",
    "    df_sub = result_df[result_df['subCategory'] == sub_cat]\n",
    "    top_articles = df_sub['articleType'].value_counts().nlargest(3).index.tolist()\n",
    "    dataset = pd.concat([dataset, df_sub[df_sub['articleType'].isin(top_articles)]])\n",
    "\n",
    "print(f'Dataset size: {len(dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data if not already saved\n",
    "if not os.path.exists('dados_32x32.pkl'):\n",
    "    class_names = [\"Tshirts\", \"Shirts\", \"Kurtas\", \"Jeans\", \"Shorts\", \"Trousers\",\n",
    "                   \"Handbags\", \"Backpacks\", \"Clutches\", \"Earrings\", \"Pendant\",\n",
    "                   \"Necklace and Chains\", \"Casual Shoes\", \"Sports Shoes\", \"Heels\",\n",
    "                   \"Sandals\", \"Sports Sandals\", \"Flip Flops\"]\n",
    "    fine_to_index = {name: idx for idx, name in enumerate(class_names)}\n",
    "    \n",
    "    train_df, test_df = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "    train_path = f'{path}/fashion-dataset/images/'\n",
    "    \n",
    "    x_train, y_train, x_test, y_test = [], [], [], []\n",
    "    id_to_index = {image_id: index for index, image_id in enumerate(dataset['id'])}\n",
    "    \n",
    "    for image_id in dataset['id']:\n",
    "        image_path = os.path.join(train_path, f'{image_id}.jpg')\n",
    "        if os.path.exists(image_path):\n",
    "            img = Image.open(image_path).convert(\"RGB\").resize((32, 32))\n",
    "            img_array = np.array(img)\n",
    "            dataset_index = id_to_index[image_id]\n",
    "            label = fine_to_index[dataset.iloc[dataset_index]['articleType']]\n",
    "            \n",
    "            if image_id in train_df['id'].values:\n",
    "                x_train.append(img_array)\n",
    "                y_train.append(label)\n",
    "            elif image_id in test_df['id'].values:\n",
    "                x_test.append(img_array)\n",
    "                y_test.append(label)\n",
    "    \n",
    "    x_train = np.array(x_train).transpose(0, 3, 1, 2).astype(\"float32\")\n",
    "    x_test = np.array(x_test).transpose(0, 3, 1, 2).astype(\"float32\")\n",
    "    x_train = (x_train - np.mean(x_train)) / np.std(x_train)\n",
    "    x_test = (x_test - np.mean(x_test)) / np.std(x_test)\n",
    "    \n",
    "    with open(\"dados_32x32.pkl\", \"wb\") as f:\n",
    "        pickle.dump({\"x_train\": x_train, \"x_test\": x_test, \"y_train\": np.array(y_train), \"y_test\": np.array(y_test)}, f)\n",
    "    print('Data saved to dados_32x32.pkl')\n",
    "else:\n",
    "    print('Data file already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload data to S3\n",
    "s3_data = sess.upload_data(path='dados_32x32.pkl', bucket=bucket, key_prefix=f'{prefix}/data')\n",
    "print(f'Training data uploaded to: {s3_data}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BA-CNN Training with SageMaker\n",
    "\n",
    "Launch training job on GPU instance. You can close this notebook - job runs independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure SageMaker TensorFlow estimator\n",
    "estimator = TensorFlow(\n",
    "    entry_point='train.py',\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.g4dn.xlarge',  # GPU instance\n",
    "    framework_version='2.13',\n",
    "    py_version='py310',\n",
    "    hyperparameters={\n",
    "        'batch_size': 128,\n",
    "        'epochs': 100\n",
    "    },\n",
    "    output_path=f's3://{bucket}/{prefix}/output',\n",
    "    code_location=f's3://{bucket}/{prefix}/code',\n",
    "    base_job_name='fashion-bacnn'\n",
    ")\n",
    "\n",
    "print('Estimator configured')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training job\n",
    "# Training runs on ml.g4dn.xlarge (GPU) - you can close this notebook\n",
    "estimator.fit({'training': s3_data}, wait=False)\n",
    "print(f'Training job started: {estimator.latest_training_job.name}')\n",
    "print('Monitor in SageMaker Console > Training > Training jobs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model artifacts\n",
    "print(f'Model artifacts: {estimator.model_data}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
